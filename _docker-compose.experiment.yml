services:
  inference:
    build: 
      context: ./app
      dockerfile: _inference/Dockerfile
    shm_size: '1gb'
    ports:
      - "18000:8000"
      - "18001:8001"
      - "18002:8002"
    volumes:
      - /data/models:/data/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    restart: unless-stopped
